\documentclass[12pt, leqno]{article} %% use to set typesize
\input{common}

\begin{document}
\hdr{2019-05-29}{2019-06-04}

\paragraph*{1: Latent semantic indexing in NIPS}
The UCI repository of data sets for machine learning includes several
``bag of words'' examples:
\begin{center}
  \url{https://archive.ics.uci.edu/ml/datasets/Bag+of+Words}
\end{center}
We are going to use latent semantic indexing to look for words in the
NIPS data set (in the files
\href{https://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.nips.txt.gz}{\tt
  docword.nips.txt.gz} and
\href{https://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/vocab.nips.txt}{\tt vocab.nips.txt})

Using the code in the repository or your own code, apply latent
semantic indexing to find the top five documents most relevant to the
query term ``circuit.''  To do this, you should
\begin{itemize}
\item Load the data into a matrix.
\item Normalize the raw word counts into TF-IDF scores.
\item Compute the truncated SVD (use 20 factors).
\item Find the index in the vocabulary associated with the word ``circuit.''
\item Compute the scores associated with the word ``circuit.''
\item Sort the documents in ascending order by scores.
\item Print the top words associated with the leading documents.
\end{itemize}
We have provided Octave codes for loading the data ({\tt load\_docwords}),
computing TF-IDF scores ({\tt tf\_idf}), and showing the top words
associated with a document ({\tt show\_top\_words}).  You will want
to use the {\em sparse} SVD command (e.g. {\tt svds}) to compute the
dominant part of the singular value decomposition.  Note that in the
code provided, we have one document per row, unlike in the notes where
we used one document per column.


\paragraph*{2: Clustering by ID}
Consider a data set consisting of $k$ clusters in $n$-dimensional
space with $n > k$.  These clusters might conventionally be recovered
by $k$-means iteration, but if they are sufficiently well separated,
they can also be computed via an interpolative decomposition; if
$A \in \bbR^{n \times m}$ is the data set (consisting of $m$) points,
then we have the approximate rank $k$ decomposition
\[
  A \approx C T
\]
where $C \in \bbR^k$ is a subset of representative columns of $A$,
and the location of the largest element in each column of $T$ indicates the
cluster to which the corresponding column of $A$ belongs.  Complete
the code in {\tt cluster\_demo} to illustrate that this works (note
that computing via pivoted QR without later clean-up is fine).


\end{document}
