\documentclass[12pt, leqno]{article} %% use to set typesize
\input{common}

\begin{document}
\hdr{2019-06-06}

% Basic tasks: parameters, hypers, prediction, predictive variance
% Learning parameters and hypers (small n)
% Scalability bottlenecks
% Smooth kernels, pivoted Cholesky, and linear solves
% Log determinant computations
% Fast MVMs

\section{The basic tasks}

In the past two lectures, we have discussed various interpretations of
kernels and how they are used for interpolation and regression.  In
today's lecture, we focus on the numerical methods used to fit
kernel-based models and to use them for prediction and uncertainty
estimation.  More specifically, we consider four problems:
\begin{itemize}
\item
  For a fixed kernel, how do we fit the {\em parameters} for a
  kernel-based model?  That is, how do we compute $c$ such that
  \[
    \bar{f}(x) = \sum_{j=1}^n c_j k(x, x_j)
  \]
  approximately satisfies $\hat{f}(x_i) \approx y_i$, where $(x_i, y_i)$
  are our observed data points?
\item
  Many kernels involve {\em hyper-parameters}, such as length scale
  and smoothness parameters.  How should we determine these hyper-parameters?
\item
  How do we quickly evaluate the predicted values $\hat{f}(x)$ at
  new points $x$?
\item
  How do we quickly evaluate measures that quantify uncertainty,
  such as the predictive variance?
\end{itemize}
Because fitting model parameters and hyper-parameters is more costly
than evaluating the predicted mean field or predictive variance, we
will focus most of our attention on the fitting tasks.  We will also
focus primarily on the probabilistic interpretation of kernels in
terms of Gaussian processes.  Because we have in mind downstream tasks
in which a user can adaptively obtain new data (as opposed to fitting
to a fixed data set), we will also discuss incremental fitting.

\section{Learning at small $n$}

We initially consider the case where there are few enough data points
that it makes sense to use standard direct factorization methods to solve the
fitting problem.  These standard factorizations require $O(n^3)$ time,
but if we use well-tuned libraries, the constants need not be large.
Using a standard laptop, we can work with data sets of size greater
than $n = 1000$ as a matter of routine.  The direct
factorization approach is also useful as a starting point for scalable
algorithms more appropriate to large $n$.

\subsection{Cholesky and kernel-only fitting}

The Cholesky factorization of a positive definite kernel matrix $K$ is
\[
  K = R^T R
\]
where $R$ is upper triangular.  The fitting system in the case of a
positive definite kernel with no tail is
\[
  Kc = y,
\]
and the standard solution algorithm is a Cholesky factorization of $K$
followed by two triangular solves
\[
  c = R^{-1} (R^{-T} y).
\]
The Cholesky factorization costs $O(n^3)$; the triangular solves each
cost $O(n^2)$.  Hence, the cost is dominated by the cost of the
Cholesky factorization.

For computation, it is useful to think of the decomposition in block terms
\[
  \begin{bmatrix} K_{11} & K_{12} \\ K_{12}^T & K_{22} \end{bmatrix} =
  \begin{bmatrix} R_{11}^T & 0 \\ R_{12}^T & R_{22}^T \end{bmatrix}
  \begin{bmatrix} R_{11} & R_{12} \\ 0 & R_{22} \end{bmatrix}.
\]
Rearranging block-by-block, we have
\begin{align*}
  K_{11} &= R_{11}^T R_{11} &
  R_{11}^T R_{11} &= K_{11} \\
  K_{12} &= R_{11}^T R_{12} &
  R_{12} &= R_{11}^{-T} K_{12} \\
  K_{22} &= R_{12}^T R_{12} + R_{22}^T R_{22} &
  R_{22}^T R_{22} &= K_{22} - R_{12}^T R_{12}
\end{align*}
That is, we can think of the decomposition in terms of a decomposition
of the leading submatrix of $K$, a triangular solve to get an
off-diagonal block, and then decomposition of the trailing {\em Schur
  complement}
\[
  S_{22} = R_{22}^T R_{22} = K_{22} - R_{12}^T R_{12} = K_{22} - K_{21} K_{11}^{-1} K_{12}.
\]
The Schur decomposition has a meaning independent of the Cholesky
factorization, as we can see by solving the system
\[
  \begin{bmatrix} R_{11}^T & 0 \\ R_{12}^T & R_{22} \end{bmatrix}
  \begin{bmatrix} R_{11} & R_{12} \\ 0 & R_{22} \end{bmatrix} \\
  \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} =
  \begin{bmatrix} 0 \\ I \end{bmatrix}.
\]
Block forward substitution gives
\[
  \begin{bmatrix} R_{11} & R_{12} \\ 0 & R_{22} \end{bmatrix}
  \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} =
  \begin{bmatrix} 0 \\ R_{22}^{-T} \end{bmatrix}
\]
and block back substitution then yields
\[
  \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} =
  \begin{bmatrix} -R_{11}^{-1} R_{12} R_{22}^{-1} R_{22}^{-T}
    \\ R_{22}^{-1} R_{22}^{-T} \end{bmatrix} =
  \begin{bmatrix} -S_{11}^{-1} K_{12} S_{22}^{-1} \\ S_{22}^{-1} \end{bmatrix}.
\]
That is, the inverse of the Schur complement is a submatrix of the
inverse of the original matrix
$K$: $S_{22}^{-1} = \begin{bmatrix} K^{-1} \end{bmatrix}_{22}$.  In
the Gaussian process setting, if $K$ is the covariance matrix, then
$K^{-1}$ is the {\em precision matrix}, and $S_{22}^{-1}$ is the
precision matrix of the posterior for the second block of variables
conditioned on observations of data points from the first block.

In introductory numerical methods courses, one typically first
encounters the (scalar\footnote{The {\em scalar} version of the
  algorithm works one column at a time.  Various {\em blocked} versions
  of the factorization algorithm update in blocks of several columns
  at a time, with a small (scalar) Cholesky factorization for the
  diagonal blocks.  Block algorithms have the same complexity as the
  corresponding scalar algorithms, but achieve better performance on
  modern hardware because they make better use of the cache.
}) {\em right-looking} version of the Cholesky algorithm:
\begin{lstlisting}
  % Overwrite K with the Cholesky factor R
  for j = 1:n
    K(j,j) = sqrt(K(j,j));
    K(j,j+1:end) = K(j,j+1:end) / K(j,j);
    K(j+1:end,j) = 0;
    K(j+1:end,j+1:end) = K(j+1:end,j+1:end) - K(j,j+1:end)'*K(j,j+1:end);
  end
\end{lstlisting}
At step $j$ of the loop, we have computed the first $j-1$ rows of $R$,
and the trailing submatrix contains the Schur complement.  At the end
of the step, we perform a rank 1 update to get a new (smaller) Schur
complement matrix.

An alternate organization, the {\em left-looking} version,
defers the Schur complement update until it is needed:
\begin{lstlisting}
  % Overwrite K with the Cholesky factor R
  K(1,1) = sqrt(K(1,1));
  K(2:end,1) = 0;
  for j = 2:n
    K(1:j-1,j) = K(1:j-1,1:j-1) \ K(1:j-1,j)
    K(j,1:j-1) = 0;
    K(j,j) = sqrt(K(j,j) - K(1:j-1,j)'*K(1:j-1,j));
  end
\end{lstlisting}
Both organizations of Cholesky do the same operations, but in a
different order.  Both require $O(n^3)$ time; the dominant
cost per step for the right-looking variant is the rank-1 update
of the Schur complement, where the dominant cost per step for the
left-looking variant is the triangular solve.

The advantage of the left-looking factorization is that it can be
applied {\em incrementally}.  That is, suppose that $K_{11}$
corresponds to the kernel matrix associated with an initial sample
of data points, and we have computed $K_{11} = R_{11}^T R_{11}$.
Then to add a second set of data points, we can do a left-looking
block update
\begin{align*}
  R_{12} &= R_{11}^{-T} K_{12} \\
  R_{22}^T R_{22} &= K_{22} - R_{12}^T R_{12}
\end{align*}
The cost of this update is $O((n_1^2 + n_2^2) n_2)$, which is
significantly less than the $O((n_1 + n_2)^3)$ cost of recomputing the
factorization from scratch in the case that $n_2 \ll n_1$.

\subsection{Fitting with a tail}

We now consider fitting in the presence of a tail term.  Polynomial
tails are typically used with conditionally positive definite kernel
functions, but they can also be used with positive definite kernels
--- and indeed they usually are used in some geostatistical
applications.  One can also incorporate non-polynomial terms into
the tail if they are useful to the model.

The fitting problem with a tail looks like
\[
  \hat{f}(x) = \sum_i c_i k(x,x_i) + \sum_j d_j p_j(x)
\]
where the coefficients $d$ satisfy the discrete orthogonality
condition
\[
  \sum_i p_j(x_i) d_i = 0
\]
for each basis function $p_j(x)$.  This gives us the linear system
\[
  \begin{bmatrix} K & P \\ P^T & 0 \end{bmatrix}
  \begin{bmatrix} c \\ d \end{bmatrix} =
  \begin{bmatrix} y \\ 0 \end{bmatrix}.
\]
We can also see this linear system from the perspective of constrained
optimization: we are minimizing a quadratic objective (associated with $K$)
subject to linear constraints (associated with $P$).  Note that this
formulation is only well posed if $P$ is full rank, a condition
sometimes known as unisolvency of the interpolation points.

One way to deal with the tail term is the ``null space'' approach;
that is, rather than adding equations that enforce the discrete
orthogonality constraint, we find a coordinate system in which the
discrete orthogonality constraint is automatic.  Specifically, suppose
we write the full QR decomposition of $P$ as
\[
  P = \begin{bmatrix} Q_1 & Q_2 \end{bmatrix}
      \begin{bmatrix} R_1 \\ 0 \end{bmatrix}.
\]
Then the constraint $P^T c = 0$ can be rewritten as $c = Q_2 w$,
giving us
\[
  Q_2^T K Q_2 w = Q_2^T y
\]
where $Q_2^T K Q_2$ is generally symmetric and positive definite even
for a conditionally positive definite kernel.  Once we have computed
$w$ (and from there $c$), we can compute $d$ by the relation
\[
  R_1 d = Q_1^T (y-Kc).
\]

An alternate approach is to partition the data points into two
groups, the first of which is unisolvent and has the same size as
the dimension of the tail.
Then we can write any $c$ satisfying $P^T c = 0$ in the form
\[
  \begin{bmatrix} c_1 \\ c_2 \end{bmatrix} =
  \begin{bmatrix} -W^T \\ I \end{bmatrix} c_2
\]
where $W = P_2 P_1^{-1}$.  Substituting this into the constrained
optimization of $c^T K c$ gives the reduced problem
\[
  \tilde{K}_{22} c_2 = (K_{22} - W K_{12} - K_{21} W^T + W K_{11} W) c_2 = y_2,
\]
from which we can recover the remaining coefficients by solving
\begin{align*}
  c_1   &= -W^T c_2 \\
  P_1 d & = y_1 - K_{11} c_1 - K_{12} c_2.
\end{align*}
This reduction is formally equivalent to Gaussian elimination and
substitution on the system
\[
  \begin{bmatrix}
    0 & P_1^T & P_2^T \\
    P_1 & K_{11} & K_{12} \\
    P_2 & K_{21} & K_{22}
  \end{bmatrix}
  \begin{bmatrix} d \\ c_1 \\ c_2 \end{bmatrix} =
  \begin{bmatrix} 0 \\ y_1 \\ y_2 \end{bmatrix}.
\]
As with the left-looking factorization described in the previous
section, we can form $W$, $K$, $\tilde{K}_{22}$, and the Cholesky
factorization of $\tilde{K}_{22}$ incrementally as new data points are added.

\subsection{Likelihoods and gradients}

Recall from the last lecture that the log likelihood function for a
(mean zero) Gaussian process is
\[
\mathcal{L} =
  -\frac{1}{2} y^T K^{-1} y - \frac{1}{2} \log \det K - \frac{n}{2} \log(2\pi).
\]
Given a Cholesky factorization $K = R^T R$, we can rewrite this as
\[
\mathcal{L} =
  - \frac{1}{2} \|R^{-T} y\|^2 - \log \det R - \frac{n}{2} \log(2 \pi),
\]
and note that
\[
  \log \det R = \sum_{i} \log r_{ii}.
\]
The cost of evaluating the log likelihood is dominated by the cost of
the initial Cholesky factorization.

In order to optimize kernel hyper-parameters via maximum likelihood,
we would also like to compute the gradient (and maybe the Hessian)
with respect to the hyper-parameters.  Recall from last time that we
computed the derivative
\[
  \delta \mathcal{L} =
    \frac{1}{2} c^T [\delta K] c - \frac{1}{2} \tr(K^{-1} \delta K)
\]
where $Kc = y$.  Unfortunately, I know of no tricks to exactly compute
$\tr(K^{-1} \delta K)$ for arbitrary $\delta K$ without in time less than
$O(n^3)$ without exploiting additional structure beyond what we have
discussed so far.

Simply computing gradients of the log likelihood is sufficient for
gradient descent or quasi-Newton methods such as BFGS.  However,
if the number of hyper-parameters is not too great, we may also decide
to compute second derivatives and compute a true Newton iteration.
Let $\theta$ be the vector of hyper-parameters and use $[f]_{,j}$
to denote the partial derivative of an expression $f$ with respect
to $\theta_j$; then
\begin{align*}
  \left[ y^T K^{-1} y \right]_{,ij}
  &= \left[ -y^T K^{-1} K_{,i} K^{-1} y
     \right]_{,j} \\
  &= 2 y^T K^{-1} K_{,i} K^{-1} K_{,j} K^{-1} y-
       y^T K^{-1} K_{,ij} K^{-1} y \\
  &= 2 c^T K_{,i} K^{-1} K_{,j} c -
     c^T K_{,ij} c \\
  \left[ \log \det K \right]_{,ij}
  &= 
  \left[
    \tr\left( K^{-1} K_{,i} \right)
  \right]_{,j} \\
  &=
  \tr\left(
    K^{-1} K_{,ij} -
    K^{-1} K_{,i} K^{-1} K_{,j}
  \right) \\
  \mathcal{L}_{,ij}  
  &=
  \frac{1}{2} c^T K_{,ij} c - c^T K_{,i} K^{-1} K_{,j} c -
  \frac{1}{2} \tr\left( K^{-1} K_{,ij} - K^{-1} K_{,i} K^{-1} K_{,j} \right).
\end{align*}
Barring tricks that take advantage of further structure in the kernel
matrix $K$ or its derivatives, computing these second partials has the
same $O(n^3)$ complexity as computing the first derivatives.

\subsection{Optimizing the nugget}

An important special case is optimization with only a single
hyper-parameter, the noise variance term or ``nugget''
term\footnote{The term ``nugget'' comes from the use of Gaussian
  process regression in geostatistical applications.  When trying
  to use Gaussian processes to predict the location of precious
  mineral deposits based on the mineral content in bore measurements,
  it is necessary to account for the possibility that a measurement
  may accidentally happen on a mineral nugget.}
That is, we seek to maximize
\[
  \mathcal{L}(\eta) =
    -\frac{1}{2} y^T (K + \eta I)^{-1} y - \frac{1}{2} \log \det (K + \eta) -
     \frac{n}{2} \log(2\pi).
\]
In this case, rather than doing a Cholesky factorization, we may
choose to compute an eigenvalue decomposition $K = Q \Lambda Q^T$
in order to obtain
\[
  \mathcal{L}(\eta) =
    -\frac{1}{2} \hat{y}^T (\Lambda + \eta I)^{-1} \hat{y}
    -\frac{1}{2} \log \det (\Lambda + \eta I) -
    \frac{n}{2} \log(2\pi)
\]
where $\hat{y} = Q^T y$.  In this case, the stationary points
satisfy a rational equation in $\eta$:
\[
\sum_{j=1}^n \left( \frac{\hat{y}_j^2}{(\lambda_j + \eta)^2}
                 - \frac{1}{2 (\lambda_j + \eta)} \right) = 0.
\]
We can run Newton on this equation in $O(n)$ time per step.

\section{Smooth kernels and low-rank structure}

The cost of parameter fitting for a general kernel matrix is $O(n^3)$,
and the cost of hyper-parameter fitting by a gradient-based method
applied to maximization of the log-likelihood involves an
additional $O(n^3)$ cost per hyper-parameter per step.  However,
for {\em smooth} kernels, the kernel matrix (without a nugget term)
may be effectively very low rank.  This case is of enough interest
that we give it special treatment here.  More specifically, we assume
a kernel matrix of the form
\[
  \tilde{K} = K + \eta I
\]
where most of the eigenvalues of $K$ are much less than $\eta$.  In
this case, we can solve the fitting problem and the computation of the
log-likelihood and gradients in much less than $O(n^3)$ time.

\subsection{Kernel-only fitting}

We begin with the solution of the linear system
\[
  (K + \eta I) c = y
\]
via an outer product factorization of $K$.  Specifically, we consider
the pivoted Cholesky factorization
\[
  \Pi^T K \Pi = R^T R
\]
where the diagonal elements of $R$ appear in descending order and
the off-diagonal elements in each row of $R$ are dominated by the
diagonal element.  The (left-looking) pivoted Cholesky algorithm
looks very much like the ordinary left-looking Cholesky algorithm,
except that we need to maintain the diagonal of the Schur complement
matrix explicitly:
\begin{lstlisting}
  piv = 1:n;        % Keep track of permutation
  diagS = diag(K);  % Keep track of the diagonal
  for j = 1:n

    % Find largest diagonal element in Schur complement
    [sjj, idxj] = max(diagS(j:end));
    idx = idx + j - 1;

    % Swap with row/col j
    if idx ~= j
      piv([j, idx]) = piv([idx, j]);
      K([j, idx], :) = K([idx, j], :);
      K(:, [j, idx]) = K(:, [idx, j]);
    end

    % Extend the Cholesky factor
    K(j,j) = sqrt(sjj);
    K(j,1:j-1) = K(j,1:j-1) / K(j,j);
    
    % Update the diagonal 
  end
\end{lstlisting}

For positive definite matrices, the trace is the same as the nuclear
norm, which dominates the Frobenius norm.  So we can easily bound the
Frobenius norm error associated with

\subsection{Fitting with a tail}

\subsection{Likelihoods and gradients}

\subsection{Beyond low rank}

\section{From factorization to iteration}

\subsection{Kernel selection and fast MVMs}

\subsection{Krylov subspaces and function approximation}

\section{Fast MVMs and scalable methods}

\subsection{Preconditioned Conjugate Gradients (PCG)}

\subsection{Stochastic trace and diagonal estimation}

\end{document}



\section{Regularization in kernel methods}

Last time, we discussed kernel methods for {\em interpolation}:
given $f_X$, we seek an approximation $\hat{f}_X$ such that
$\hat{f}_X = f_X$.  However, the kernel matrix $K_{XX}$ is sometimes
very nearly ill-conditioned, particularly when the underlying kernel
is smooth (e.g.~a squared exponential kernel).  This near-singularity
means that when $f$ is not smooth enough to lie in the native space
$\mathcal{H}$ for the kernel, or when our measurements are
contaminated with some error, kernel interpolation schemes may not
give accurate results.  And even when $f$ is in the appropriate native
space and we are able to evaluate $f$ exactly, this lack of stability
may cause problems because of the influence of rounding errors.

We deal with the problem of instability in kernel methods the same way
we deal with instability in other fitting problems: we
{\em regularize}.  There are many ways that we might choose to
regularize, but we will focus on the common Tikhonov regularization
approach.  As always in kernel methods, there are multiple stories for
the same method; we will tell two of them.

\subsection{Feature space and kernel ridge regression}

Recall the feature space version of kernel interpolation: write
\[
\hat{f}(x) = \psi(x)^T c
\]
where $c$ is determined by the problem
\[
  \mbox{minimize } \|c\|^2 \mbox{ s.t.~} \Psi^T c = f_X
\]
with $\Psi$ the matrix whose columns are feature vectors at the data
points in $X$.
{\em Kernel ridge regression} instead solves the unconstrained
minimization problem
\[
  \mbox{minimize } \lambda \|c_{\lambda}\|^2 + \|\Psi^T c_{\lambda} - f_X\|^2
\]
That is, rather than enforcing the interpolation constraints, we
minimize a combination of a regularity term (the norm of
$c_{\lambda}$) and a data fidelity term.  As the weight $\lambda$ goes
to zero, we recover kernel interpolation.  For nonzero $\lambda$,
we solve the critical point equations
\[
  \lambda c_{\lambda} + \Psi (\Psi^T c_{\lambda} - f_x) = 0,
\]
which we may rewrite using $r = \Psi^T c_\lambda -f_X$ as
\[
  \begin{bmatrix}
    \lambda I & \Psi \\
    \Psi^T & -I
  \end{bmatrix}
  \begin{bmatrix} c_\lambda \\ r \end{bmatrix} =
  \begin{bmatrix} 0 \\ f_X \end{bmatrix}.
\]
Eliminating $c_{\lambda}$ gives the equation
\[
  -(I+\lambda^{-1} \Psi^T \Psi) r = f_X,
\]
and back-subsittution yields
\[
  \lambda c_{\lambda} = \Psi (I+\lambda^{-1} \Psi^T \Psi)^{-1} f_X.
\]
Dividing both sides by $\lambda$, we have
\[
  \hat{f}_{\lambda}(x) = \psi(x)^T c_{\lambda} =
    \psi(x)^T \Psi (\Psi^T \Psi + \lambda I)^{-1} f_X,
\]
and applying the kernel trick gives
\[
  \hat{f}_{\lambda}(x) = k_{xX} (K_{XX} + \lambda I)^{-1} f_X.
\]

As with kernel interpolation, the story of kernel ridge regression can
be told without reference to a particular basis or feature map.
Observe as before that $\Psi^T c_{\lambda} = \hat{f}_{\lambda,X}$ and that
$\|c_{\lambda}\| = \|\hat{f}\|_{\mathcal{H}}^2$ for an appropriate
reproducing kernel Hilbert space.  The kernel ridge regression problem
is therefore
\[
  \mbox{minimize } \lambda \|\hat{f}\|_{\mathcal{H}}^2 + \|\hat{f}_X -f_X\|^2
  \mbox{ over } \hat{f} \in \mathcal{H}.
\]

\subsection{GPs with noise}

The interpretation of Tikhonov regularization for Gaussian processes
is straightforward.  Suppose that $f$ is drawn from a GP with mean
zero and covariance kernel $K$, and we wish to compute a marginal
distribution conditioned on knowing $y = f_X + u$ where
$u$ is a vector of independent Gaussian random variables with zero
mean and variance $\sigma^2$.  Then the posterior distribution
for $f$ conditioned on the data is a GP with mean and covariance
\begin{align*}
  \hat{\mu}(x) &= k_{xX} \tilde{K}^{-1} f_X, &
  \hat{k}(x,x') &= k(x,x') - k_{xX} \tilde{K}^{-1} k_{Xx'}
\end{align*}
where $\tilde{K} = K + \sigma^2 I$.  The derivation comes from looking
at the multivariate Gaussian distribution of the data ($y$) together
with function values at locations of interest.

\section{Choosing hyperparameters}

Whether we call it kernel ridge regression or GP regression with
noise, Tikhonov regularization involves a free parameter --- which may
come in addition to other hyper-parameters in the kernel, like length
scale or scale factor.  How do we choose all these hyper-parameters?
There are several methods, though all sometimes fail, and none of
them is uniformly the best.  We will discuss four approaches:
the discrepancy principle, the L-curve, cross-validation, and maximum
likelihood estimation.

\subsection{The discrepancy principle and the L-curve}

The {\em discrepancy principle} (due to Morozov) says that we should
choose the regularization parameter based on the variance of the
noise.  This may be useful when the primary source of error comes from
measurement noise from instruments (for example), but often we do not
have this information; what shall we do then?  Also, the discrepancy
principle does not tell us what to do with other hyper-parameters,
such as kernel length scales.

The {\em L-curve} is a graphical plot on a log-log axis of the norm of
the data fidelity term versus the norm of the regularization term.
Often such plots have a ``corner'' associated with the favored choice
$\lambda_*$ for the regularization parameter.  Increasing $\lambda$
beyond $\lambda_*$ increases the residual error quickly, while
decreasing $\lambda$ from $\lambda_*$ has only a modest impact on the
residual error, but causes a rapid increase in the size of the
regularization term.  As with the discrepancy principle, the L-curve
is primarily used for determining a single regularization parameter,
and not for fitting other hyper-parameters such as the kernel
length-scale.

\subsection{Cross-validation}

The idea of {\em cross-validation} is to fit the method to split the
training data into two sets: a subset that we actually use to fit the
model, and a held-out set to test the generalization error.  We
usually use more than one splitting of the data to do this.  For
example, the {\em leave-one-out cross-validation} (LOOCV) statistic
for a regression method for a function $f$ on data points $X$ is
\[
  \mbox{LOOCV} = \frac{1}{n} \sum_{i=1}^n (f^{(-i)}(x_i)-f(x_i))^2
\]
where $f^{(-i)}$ refers to the model fit to all of the points in $X$
except for $x_i$.  One can do more complicated things, but the LOOCV
statistic has a lovely structure that lets us do fast computations,
and this is worth exploring.

\subsubsection{Fast LOOCV for least squares}

Before we explore the case of kernel methods, let us first consider
LOOCV for the ordinary linear least squares problem:
\[
  \mbox{minimize } \|Ax-b\|^2
\]
To compute the LOOCV statistic in the most obvious way, we would
delete each row $a_i^T$ of $A$ in turn, fit the model coefficients
$x^{(-i)}$, and then evaluate $r^{(-i)} = b_i - a_i^T x^{(-i)}$.
This involves $m$
least squares problems, for a total cost of $O(m^2 n^2)$ (as opposed
to the usual $O(mn^2)$ cost for an ordinary least squares problem).
Let us find a better way!

The key is to write the equations for $x^{(-i)}$ as a small change to
the equations for $A^T A x^* = A^T b$:
\[
  (A^T A - a_i a_i^T) x^{(-i)} = A^T b - a_i b_i.
\]
This subtracts the influence of row $i$ from both sides of the normal
equations.  By introducing the auxiliary variable $\gamma = -a_i^T x^{(-i)}$,
we have
\[
  \begin{bmatrix}
    A^TA & a_i \\
    a_i^T & 1
  \end{bmatrix}
  \begin{bmatrix} x^{(-i)} \\ \gamma \end{bmatrix} =
  \begin{bmatrix} A^T b - a_i b_i \\ 0 \end{bmatrix}.
\]
Eliminating $x^{(-i)}$ gives
\[
  (1-\ell_i^2) \gamma = \ell_i^2 b_i - a_i^T x^*
\]
where $\ell_i^2 = a_i^T (A^T A)^{-1} a_i$ is called the
{\em leverage score} for row $i$.  Now, observe that
if $r = b-Ax^*$ is the residual for the full problem, then
\[
(1-\ell_i^2) r^{(-i)}
  = (1-\ell_i^2) (b_i + \gamma)
  = (1-\ell_i^2) b_i + \ell_i^2 b_i - a_i^T x_*
  = r_i,
\]
or, equivalently
\[
  r^{(-i)} = \frac{r_i}{1-\ell_i^2}.
\]
We finish the job by observing that $\ell_i^2$ is the $i$th diagonal
element of the orthogonal projector $\Pi = A(A^TA)A^{-1}$, which we
can also write in terms of the economy QR decomposition of $A$ as
$\Pi = QQ^T$.  Hence, $\ell_i^2$ is the squared row sum of $Q$ in
the QR factorization.

\subsubsection{Fast LOOCV for kernels}

The trick for computing the LOOCV statistic for kernels is similar to
the trick for least squares, at least in broad outlines.  Let $c$ be
the coefficient vector fit to all the nodes, and let $c^{(-i)}$ be the
coefficient vector for the expansion fit to all the nodes except node
$i$; that is, we want $c^{(-i)}_i = 0$ and we allow $r^{(-i)} = f(x_i)
- k_{xX} c^{(-i)}$ to be nonzero.  Then
\[
  \begin{bmatrix} \tilde{K} & e_i \\ e_i^T & 0 \end{bmatrix}
  \begin{bmatrix} c^{(-i)} \\ r^{(-i)} \end{bmatrix} =
  \begin{bmatrix} f_X \\ 0 \end{bmatrix},
\]
and Gaussian elimination of $c^{(-i)}$ yields
\[
  [\tilde{K}^{-1}]_{ii} r^{(-i)} = e_i^T \tilde{K}^{-1} f_X = c_i,
\]
and therefore
\[
  r^{(-i)} = \frac{c_i}{[\tilde{K}^{-1}]_{ii}}.
\]
The observent reader may notice that this yields essentially the same
argument we saw in the error analysis of kernel methods, and that
$[\tilde{K}^{-1}]_{ii}^{-1}$ is the squared power function for
evaluating the error at $x_i$ given data at all the other points.

What about the derivatives of $r^{(-i)}$ with respect to any
hyper-parameters?  After all, these are important if we are going to
optimize.  We know that
\[
  \delta[\tilde{K}^{-1}] = -\tilde{K}^{-1} [\delta \tilde{K}] \tilde{K}^{-1}
\]
and differentiating $c = K^{-1} f_X$ (and using the fact that the
function values are independent of the hyper-parameters) gives
\[
  \delta c = -\tilde{K}^{-1} [\delta \tilde{K}] c.
\]
Let $w$ denote $\tilde{K}^{-1} e_i$; then
\begin{align*}
  \delta c_i &= -w^T [\delta \tilde{K}] c, &
  \delta\left( [K^{-1}]_{ii} \right) &= -w^T [\delta \tilde{K}] w,
\end{align*}
and the quotient rule, together with a little algebra, gives
\[
\delta r^{(-i)} = \frac{[\delta \tilde{K} w]^T (wc_i-cw_i)}
                      {[\tilde{K}^{-1}]_{ii}^2}.
\]

\subsection{Maximum likelihood estimation}

Finally, we consider the {\em maximum likelihood estimation} scheme.
If we have data $y$ drawn from a distribution
$p(y ; \theta)$ where $\theta$ are unknown parameters, the idea of
maximum likelihood is to maximize $p(y; \theta)$ with respect to $\theta$.
Often the probability density is awkwardly scaled for computation,
and so we typically instead use the {\em log} likelihood
\[
  \mathcal{L}(\theta) =  \log p(y; \theta)
\]
In the case of Gaussian processes, we have
\[
p(y) = \frac{1}{\sqrt{\det(2\pi K)}}
       \exp\left( -\frac{1}{2} (y-\mu)^T K^{-1} (y-\mu) \right)
\]
and
\[
\log p(y) = -\frac{1}{2} (y-\mu)^T K^{-1} (y-\mu)
            -\frac{1}{2} \log \det(K) - \frac{n}{2} \log(2\pi)
\]
The first term is the {\em model fidelity} term; it is larger the closer
$y$ is to $\mu$ in the norm induced by $K^{-1}$, with a maximum value
of zero when $y=\mu$.  The second term is the {\em model complexity}
term; it is larger when $K$ has lower volume, i.e.~the likely model
predictions are in a smaller region.  The last term is independent
of any kernel hyper-praameters, and so is irrelevant for optimization.

With an eye to optimization, we again want to compute derivatives.
The derivative of the model fidelity term with respect to kernel
hyperparameters is straightforward:
\[
  \delta\left[ -\frac{1}{2} (y-\mu)^T K^{-1} (y-\mu) \right] =
  \frac{1}{2} c^T [\delta K] c
\]
where $c = K^{-1}(y-\mu)$.  The more interesting piece is the
derivative of the log determinant.  To get this, we observe that
\[
  \det(I+E) = \prod_{i=1}^n (1+\lambda_i(E)),
\]
and if $E$ is small, linearization about $E = 0$ gives
\[
  \det(I+E) = 1 + \sum_{i=1}^n \lambda_i(E) + O(\|E\|^2).
\]
Therefore, the derivative of the determinant at the identity
in a direction $E$ is just $\tr(E) = \sum_i \lambda_i(E) = \sum_i E_{ii}$.
We deal with the more general case by observing that
$\det(K+E) = \det(K) \det(I+K^{-1} E)$; hence,
\[
  \delta [\det(K)] = \det(K) \tr(K^{-1} \delta K).
\]
Finally, we have
\[
  \delta[\log \det(K)] = \frac{\delta[\det(K)]}{\det(K)}
    = \tr(K^{-1} \delta K).
\]

\end{document}
